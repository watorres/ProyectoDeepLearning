{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from scipy.stats import scoreatpercentile\n",
        "\n",
        "# Descargar el archivo comprimido desde GitHub\n",
        "!wget https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip\n",
        "\n",
        "# Descomprimir el archivo\n",
        "import zipfile\n",
        "with zipfile.ZipFile('device_gnss.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Cargar el archivo CSV en un DataFrame\n",
        "df = pd.read_csv('device_gnss.csv')\n",
        "\n",
        "# Seleccionar las columnas de entrada y salida\n",
        "X = df[['Cn0DbHz', 'PseudorangeRateMetersPerSecond', 'PseudorangeRateUncertaintyMetersPerSecond']]\n",
        "y = df[['SvPositionXEcefMeters', 'SvPositionYEcefMeters', 'SvPositionZEcefMeters']]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar los datos de entrada\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape de los datos para la entrada en la red convolucional\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Definir la arquitectura de la red convolucional\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3))  # Salida con 3 dimensiones (x, y, z)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# Calcular los errores de distancia horizontal\n",
        "dist_errors = np.linalg.norm(y_pred - y_test, axis=1)\n",
        "\n",
        "# Calcular los errores de percentil 50 y 95\n",
        "percentile_50 = np.percentile(dist_errors, 50)\n",
        "percentile_95 = np.percentile(dist_errors, 95)\n",
        "\n",
        "# Imprimir los resultados\n",
        "print('Error de distancia horizontal:', dist_errors)\n",
        "print('Error de percentil 50:', percentile_50)\n",
        "print('Error de percentil 95:', percentile_95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei2jlyZH4BPT",
        "outputId": "084c2ed7-cad2-493c-f3a4-cb514bde0a58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-31 22:27:52--  https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/watorres/ProyectoDeepLearning/main/device_gnss.csv.zip [following]\n",
            "--2023-05-31 22:27:53--  https://raw.githubusercontent.com/watorres/ProyectoDeepLearning/main/device_gnss.csv.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9790985 (9.3M) [application/zip]\n",
            "Saving to: ‘device_gnss.csv.zip’\n",
            "\n",
            "device_gnss.csv.zip 100%[===================>]   9.34M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-31 22:27:53 (76.4 MB/s) - ‘device_gnss.csv.zip’ saved [9790985/9790985]\n",
            "\n",
            "Epoch 1/10\n",
            "1400/1400 [==============================] - 10s 6ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "1400/1400 [==============================] - 5s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/10\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/10\n",
            "1400/1400 [==============================] - 5s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/10\n",
            "1400/1400 [==============================] - 8s 5ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/10\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/10\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/10\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/10\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "350/350 [==============================] - 1s 1ms/step\n",
            "Error de distancia horizontal: [nan nan nan ... nan nan nan]\n",
            "Error de percentil 50: nan\n",
            "Error de percentil 95: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**El resultado que obtenido muestra que el modelo no ha convergido durante el entrenamiento. Se observa que tanto la pérdida de entrenamiento (loss) como la pérdida de validación (val_loss) tienen valores nan (no numéricos), lo que indica que no se ha logrado calcular una función de pérdida válida durante el entrenamiento.**\n",
        "\n",
        "**A continuación, se modifica el código en el cual se han realizado los siguientes cambios:**\n",
        "\n",
        "**1) Se ha ajustado la tasa de aprendizaje (learning_rate) del optimizador Adam a 0.001. Puedes ajustar este valor según sea necesario.**\n",
        "\n",
        "**2) El número de épocas de entrenamiento se ha incrementado a 50. Esto permite que el modelo tenga más oportunidades de aprender durante el entrenamiento.**\n",
        "\n",
        "**3) Se ha agregado un optimizer personalizado al compilar el modelo, utilizando el valor ajustado para la tasa de aprendizaje.**"
      ],
      "metadata": {
        "id": "-05jEAp0_aOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from scipy.stats import scoreatpercentile\n",
        "\n",
        "# Descargar el archivo comprimido desde GitHub\n",
        "!wget https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip\n",
        "\n",
        "# Descomprimir el archivo\n",
        "import zipfile\n",
        "with zipfile.ZipFile('device_gnss.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Cargar el archivo CSV en un DataFrame\n",
        "df = pd.read_csv('device_gnss.csv')\n",
        "\n",
        "# Seleccionar las columnas de entrada y salida\n",
        "X = df[['Cn0DbHz', 'PseudorangeRateMetersPerSecond', 'PseudorangeRateUncertaintyMetersPerSecond']]\n",
        "y = df[['SvPositionXEcefMeters', 'SvPositionYEcefMeters', 'SvPositionZEcefMeters']]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar los datos de entrada\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape de los datos para la entrada en la red convolucional\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Definir la arquitectura de la red convolucional\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3))  # Salida con 3 dimensiones (x, y, z)\n",
        "\n",
        "# Compilar el modelo\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# Calcular los errores de distancia horizontal\n",
        "dist_errors = np.linalg.norm(y_pred - y_test, axis=1)\n",
        "\n",
        "# Calcular los errores de percentil 50 y 95\n",
        "percentile_50 = np.percentile(dist_errors, 50)\n",
        "percentile_95 = np.percentile(dist_errors, 95)\n",
        "\n",
        "# Imprimir los resultados\n",
        "print('Error de distancia horizontal:', dist_errors)\n",
        "print('Error de percentil 50:', percentile_50)\n",
        "print('Error de percentil 95:', percentile_95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S6j0rU98maX",
        "outputId": "216dfe29-7cc1-48da-9988-803c2ee9f319"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-31 22:34:11--  https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/watorres/ProyectoDeepLearning/main/device_gnss.csv.zip [following]\n",
            "--2023-05-31 22:34:11--  https://raw.githubusercontent.com/watorres/ProyectoDeepLearning/main/device_gnss.csv.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9790985 (9.3M) [application/zip]\n",
            "Saving to: ‘device_gnss.csv.zip.1’\n",
            "\n",
            "device_gnss.csv.zip 100%[===================>]   9.34M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-31 22:34:11 (74.7 MB/s) - ‘device_gnss.csv.zip.1’ saved [9790985/9790985]\n",
            "\n",
            "Epoch 1/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/50\n",
            "1400/1400 [==============================] - 5s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 11/50\n",
            "1400/1400 [==============================] - 5s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 12/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 13/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 14/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 15/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 16/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 17/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 18/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 19/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 20/50\n",
            "1400/1400 [==============================] - 5s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 21/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 22/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 23/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 24/50\n",
            "1400/1400 [==============================] - 5s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 25/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 26/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 27/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 28/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 29/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 30/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 31/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 32/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 33/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 34/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 35/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 36/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 37/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 38/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 39/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 40/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 41/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 42/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 43/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 44/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 45/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 46/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 47/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 48/50\n",
            "1400/1400 [==============================] - 5s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 49/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 50/50\n",
            "1400/1400 [==============================] - 4s 3ms/step - loss: nan - val_loss: nan\n",
            "350/350 [==============================] - 1s 2ms/step\n",
            "Error de distancia horizontal: [nan nan nan ... nan nan nan]\n",
            "Error de percentil 50: nan\n",
            "Error de percentil 95: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**De  nuevo, el resultado que obtenido muestra que el modelo no ha convergido durante el entrenamiento. Se observa que tanto la pérdida de entrenamiento (loss) como la pérdida de validación (val_loss) tienen valores nan (no numéricos), lo que indica que no se ha logrado calcular una función de pérdida válida durante el entrenamiento.**"
      ],
      "metadata": {
        "id": "Fv8B5let_7H0"
      }
    }
  ]
}