{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJHaHVqxY1KU",
        "outputId": "65bf372f-0d7c-479d-8e9e-31c556b4ffdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1341/1341 [==============================] - 9s 4ms/step - loss: 0.1046 - val_loss: 0.0984\n",
            "Epoch 2/10\n",
            "1341/1341 [==============================] - 4s 3ms/step - loss: 0.0981 - val_loss: 0.0981\n",
            "Epoch 3/10\n",
            "1341/1341 [==============================] - 4s 3ms/step - loss: 0.0973 - val_loss: 0.0987\n",
            "Epoch 4/10\n",
            "1341/1341 [==============================] - 5s 4ms/step - loss: 0.0961 - val_loss: 0.0958\n",
            "Epoch 5/10\n",
            "1341/1341 [==============================] - 4s 3ms/step - loss: 0.0949 - val_loss: 0.0961\n",
            "Epoch 6/10\n",
            "1341/1341 [==============================] - 5s 4ms/step - loss: 0.0938 - val_loss: 0.0945\n",
            "Epoch 7/10\n",
            "1341/1341 [==============================] - 4s 3ms/step - loss: 0.0927 - val_loss: 0.0928\n",
            "Epoch 8/10\n",
            "1341/1341 [==============================] - 4s 3ms/step - loss: 0.0915 - val_loss: 0.0911\n",
            "Epoch 9/10\n",
            "1341/1341 [==============================] - 6s 4ms/step - loss: 0.0901 - val_loss: 0.0915\n",
            "Epoch 10/10\n",
            "1341/1341 [==============================] - 4s 3ms/step - loss: 0.0887 - val_loss: 0.0902\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.0902\n",
            "Loss en el conjunto de prueba: 0.09020043909549713\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "Error de percentil 50: 8421.986117046305\n",
            "Error de percentil 95: 18874.204533836655\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Cargar los datos\n",
        "data_url = 'https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip'\n",
        "df = pd.read_csv(data_url, compression='zip')\n",
        "\n",
        "# Seleccionar las características relevantes\n",
        "features = ['RawPseudorangeMeters', 'RawPseudorangeUncertaintyMeters', 'SvPositionXEcefMeters',\n",
        "            'SvPositionYEcefMeters', 'SvPositionZEcefMeters', 'SvClockBiasMeters', 'SvClockDriftMetersPerSecond']\n",
        "target = ['WlsPositionXEcefMeters', 'WlsPositionYEcefMeters', 'WlsPositionZEcefMeters']\n",
        "\n",
        "df = df[features + target].dropna()\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_df.values)\n",
        "test_data = scaler.transform(test_df.values)\n",
        "\n",
        "# Preparar los datos de entrada y salida\n",
        "train_X, train_y = train_data[:, :-3], train_data[:, -3:]\n",
        "test_X, test_y = test_data[:, :-3], test_data[:, -3:]\n",
        "\n",
        "# Reshape para LSTM (n_samples, timesteps, n_features)\n",
        "timesteps = 1\n",
        "train_X = train_X.reshape((train_X.shape[0], timesteps, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], timesteps, test_X.shape[1]))\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(3))  # 3 para las coordenadas X, Y, Z\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(train_X, train_y, epochs=10, batch_size=32, validation_data=(test_X, test_y))\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "loss = model.evaluate(test_X, test_y)\n",
        "print('Loss en el conjunto de prueba:', loss)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "predictions = model.predict(test_X)\n",
        "\n",
        "# Desnormalizar las predicciones y los valores reales\n",
        "predictions = scaler.inverse_transform(np.concatenate((test_X.reshape((test_X.shape[0], -1)), predictions), axis=1))[:, -3:]\n",
        "actual = scaler.inverse_transform(np.concatenate((test_X.reshape((test_X.shape[0], -1)), test_y), axis=1))[:, -3:]\n",
        "\n",
        "# Calcular los errores de distancia\n",
        "errors = np.linalg.norm(predictions - actual, axis=1)\n",
        "percentile_50 = np.percentile(errors, 50)\n",
        "percentile_95 = np.percentile(errors, 95)\n",
        "\n",
        "print('Error de percentil 50:', percentile_50)\n",
        "print('Error de percentil 95:', percentile_95)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado muestra el progreso del entrenamiento del modelo a lo largo de 10 épocas. Cada época representa un ciclo completo de alimentación hacia adelante y hacia atrás de los datos de entrenamiento a través de la red neuronal.\n",
        "\n",
        "Para cada época, se muestra la pérdida (loss) tanto en el conjunto de entrenamiento como en el conjunto de validación (val_loss). La pérdida es una medida de la discrepancia entre las predicciones del modelo y los valores reales. El objetivo del entrenamiento es minimizar esta pérdida.\n",
        "\n",
        "En este caso, podemos observar que la pérdida disminuye progresivamente a medida que avanzan las épocas tanto en el conjunto de entrenamiento como en el conjunto de validación. Esto indica que el modelo está aprendiendo a hacer predicciones más precisas a medida que se entrena.\n",
        "\n",
        "Después de las 10 épocas, se evalúa el modelo en el conjunto de prueba y se muestra la pérdida en el conjunto de prueba (Loss en el conjunto de prueba). En este caso, la pérdida en el conjunto de prueba es 0.0902, lo que indica que el modelo tiene un buen desempeño en el conjunto de prueba.\n",
        "\n",
        "Además, se calculan los errores de distancia utilizando las predicciones del modelo y los valores reales en el conjunto de prueba. El error de percentil 50 indica que el 50% de los errores de distancia son menores o iguales a 8421.9861 metros, mientras que el error de percentil 95 indica que el 95% de los errores de distancia son menores o iguales a 18874.2045 metros. Estos valores pueden ser utilizados para evaluar la precisión del modelo en la determinación de la ubicación de los teléfonos móviles.\n",
        "\n",
        "En resumen, el modelo ha sido entrenado con éxito y muestra un buen desempeño en el conjunto de prueba, con una pérdida baja y errores de distancia aceptables. Sin embargo, es importante tener en cuenta el contexto del problema y las métricas de desempeño específicas para evaluar si estos resultados son satisfactorios para el objetivo del proyecto"
      ],
      "metadata": {
        "id": "Mhtqw-CwZczb"
      }
    }
  ]
}