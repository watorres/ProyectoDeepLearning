{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGb-4-oXZhaI",
        "outputId": "0422f9b9-6458-4f24-f815-fea63d38be3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1341/1341 [==============================] - 24s 7ms/step - loss: 7137662.5000\n",
            "Epoch 2/10\n",
            "1341/1341 [==============================] - 9s 7ms/step - loss: 7137592.5000\n",
            "Epoch 3/10\n",
            "1341/1341 [==============================] - 9s 7ms/step - loss: 7137530.5000\n",
            "Epoch 4/10\n",
            "1341/1341 [==============================] - 9s 7ms/step - loss: 7137469.5000\n",
            "Epoch 5/10\n",
            "1341/1341 [==============================] - 10s 7ms/step - loss: 7137410.0000\n",
            "Epoch 6/10\n",
            "1341/1341 [==============================] - 10s 7ms/step - loss: 7137348.5000\n",
            "Epoch 7/10\n",
            "1341/1341 [==============================] - 8s 6ms/step - loss: 7137284.5000\n",
            "Epoch 8/10\n",
            "1341/1341 [==============================] - 9s 7ms/step - loss: 7137225.0000\n",
            "Epoch 9/10\n",
            "1341/1341 [==============================] - 9s 7ms/step - loss: 7137159.5000\n",
            "Epoch 10/10\n",
            "1341/1341 [==============================] - 10s 8ms/step - loss: 7137096.5000\n",
            "336/336 [==============================] - 2s 2ms/step - loss: 7140731.0000\n",
            "Loss en el conjunto de prueba: 7140731.0\n",
            "336/336 [==============================] - 2s 2ms/step\n",
            "Error del percentil 50: 14946342.300509445\n",
            "Error del percentil 95: 23992634.87077629\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
        "\n",
        "def quantile_loss(q, y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    return tf.reduce_mean(tf.maximum(q * error, (q - 1) * error), axis=-1)\n",
        "\n",
        "# Leer el archivo CSV comprimido\n",
        "data = pd.read_csv('https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip', compression='zip')\n",
        "\n",
        "# Seleccionar las columnas relevantes para el análisis\n",
        "selected_columns = ['TimeNanos', 'SvPositionXEcefMeters', 'SvPositionYEcefMeters', 'SvPositionZEcefMeters']\n",
        "data = data[selected_columns]\n",
        "\n",
        "# Eliminar filas con valores faltantes\n",
        "data = data.dropna()\n",
        "\n",
        "# Dividir los datos en características (X) y etiquetas (y)\n",
        "X = data[['TimeNanos', 'SvPositionXEcefMeters', 'SvPositionYEcefMeters']]\n",
        "y = data['SvPositionZEcefMeters']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar los datos de entrada\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape de los datos de entrada para ser compatibles con la red neuronal recurrente\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "# Definir el modelo de red neuronal recurrente bidireccional\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(1, X_train_scaled.shape[1])))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compilar el modelo con la función de pérdida de error de percentil personalizada\n",
        "model.compile(loss=lambda y_true, y_pred: quantile_loss(0.5, y_true, y_pred), optimizer='adam')\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "loss = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Loss en el conjunto de prueba: {loss}')\n",
        "\n",
        "# Obtener las predicciones del modelo en el conjunto de prueba\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# Calcular el error del percentil 50 y 95\n",
        "error_percentil_50 = np.percentile(np.abs(y_pred.ravel() - y_test), 50)\n",
        "error_percentil_95 = np.percentile(np.abs(y_pred.ravel() - y_test), 95)\n",
        "\n",
        "print(f'Error del percentil 50: {error_percentil_50}')\n",
        "print(f'Error del percentil 95: {error_percentil_95}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, el modelo parece tener una función de pérdida relativamente alta, en el rango de millones. Esto significa que el modelo aún no ha alcanzado un buen ajuste a los datos y puede requerir más entrenamiento o ajustes en la arquitectura/modelo.\n",
        "\n",
        "El error del percentil 50 y 95 muestra el rango de errores en las predicciones del modelo. El error del percentil 50 es el valor en el medio, mientras que el error del percentil 95 es el valor en el percentil 95, lo que significa que el 95% de los errores se encuentran por debajo de ese valor."
      ],
      "metadata": {
        "id": "TCO8quXIe41t"
      }
    }
  ]
}