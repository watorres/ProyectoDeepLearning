{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqIi6SWajlVL",
        "outputId": "83fcdc95-9eef-45a0-9378-a866eb789fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1341/1341 [==============================] - 23s 11ms/step - loss: 256431072739328.0000 - val_loss: 256320930316288.0000\n",
            "Epoch 2/10\n",
            "1341/1341 [==============================] - 6s 5ms/step - loss: 256428707151872.0000 - val_loss: 256319034490880.0000\n",
            "Epoch 3/10\n",
            "1341/1341 [==============================] - 7s 5ms/step - loss: 256427062984704.0000 - val_loss: 256317524541440.0000\n",
            "Epoch 4/10\n",
            "1341/1341 [==============================] - 6s 5ms/step - loss: 256425334931456.0000 - val_loss: 256315863597056.0000\n",
            "Epoch 5/10\n",
            "1341/1341 [==============================] - 7s 5ms/step - loss: 256423506214912.0000 - val_loss: 256313934217216.0000\n",
            "Epoch 6/10\n",
            "1341/1341 [==============================] - 6s 4ms/step - loss: 256421945933824.0000 - val_loss: 256312139055104.0000\n",
            "Epoch 7/10\n",
            "1341/1341 [==============================] - 7s 5ms/step - loss: 256420352098304.0000 - val_loss: 256310427779072.0000\n",
            "Epoch 8/10\n",
            "1341/1341 [==============================] - 6s 5ms/step - loss: 256418489827328.0000 - val_loss: 256308783611904.0000\n",
            "Epoch 9/10\n",
            "1341/1341 [==============================] - 7s 5ms/step - loss: 256416711442432.0000 - val_loss: 256307139444736.0000\n",
            "Epoch 10/10\n",
            "1341/1341 [==============================] - 7s 5ms/step - loss: 256414865948672.0000 - val_loss: 256305226842112.0000\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "Error Percentile 50: 26714063.067030497\n",
            "Error Percentile 95: 29601549.566716768\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense\n",
        "\n",
        "# Descargar y cargar el archivo CSV con los datos\n",
        "data_url = 'https://github.com/watorres/ProyectoDeepLearning/raw/main/device_gnss.csv.zip'\n",
        "data = pd.read_csv(data_url, compression='zip')\n",
        "\n",
        "# Preprocesar los datos\n",
        "data = data[['TimeNanos', 'RawPseudorangeMeters', 'SvPositionXEcefMeters', 'SvPositionYEcefMeters', 'SvPositionZEcefMeters']]\n",
        "data = data.dropna()\n",
        "X = data[['TimeNanos', 'RawPseudorangeMeters']]\n",
        "y = data[['SvPositionXEcefMeters', 'SvPositionYEcefMeters', 'SvPositionZEcefMeters']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Implementación personalizada de la capa de atención\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], 1), initializer='normal', trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        score = tf.matmul(inputs, self.W)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        weighted_sum = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
        "        return weighted_sum\n",
        "\n",
        "# Definir y entrenar el modelo\n",
        "input_layer = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
        "conv1d_layer = Conv1D(64, 1, activation='relu')(input_layer)\n",
        "lstm_layer = LSTM(64, return_sequences=True)(conv1d_layer)\n",
        "attention_layer = Attention()(lstm_layer)\n",
        "output_layer = Dense(3)(attention_layer)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Calcular el error para los percentiles 50 y 95\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "errors = np.sqrt(np.sum((y_pred - y_test.values)**2, axis=1))\n",
        "percentile_50 = np.percentile(errors, 50)\n",
        "percentile_95 = np.percentile(errors, 95)\n",
        "\n",
        "print(\"Error Percentile 50:\", percentile_50)\n",
        "print(\"Error Percentile 95:\", percentile_95)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Estos resultados indican que el modelo ha sido entrenado durante 10 épocas. Durante el entrenamiento, se muestra el valor de la función de pérdida (loss) tanto para el conjunto de entrenamiento como para el conjunto de validación (val_loss).\n",
        "\n",
        "En cada época, se muestra el valor de la función de pérdida tanto para el conjunto de entrenamiento como para el conjunto de validación. En este caso, los valores de pérdida son bastante altos, lo que indica que el modelo no está aprendiendo correctamente los patrones en los datos.\n",
        "\n",
        "Después del entrenamiento, se calculan los errores para los percentiles 50 y 95 en el conjunto de prueba. El error del percentil 50 (Error Percentile 50) es de aproximadamente 26,714,063.07, lo que significa que el 50% de los errores se encuentran por debajo de este valor. El error del percentil 95 (Error Percentile 95) es de aproximadamente 29,601,549.57, lo que significa que el 95% de los errores se encuentran por debajo de este valor.\n",
        "\n",
        "En resumen, los resultados indican que el modelo no está obteniendo un buen desempeño en la tarea de predicción, ya que la función de pérdida es alta y los errores son significativos."
      ],
      "metadata": {
        "id": "eXJpRdR4lJ8r"
      }
    }
  ]
}